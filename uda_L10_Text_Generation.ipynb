{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uda_L10_Text_Generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNranr1+dY+OrLPumrBghE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat2ram/Keras-and-Tensorflow/blob/master/uda_L10_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX-2T-z0OJoc"
      },
      "source": [
        "#############Not able to run program due to RAM issue################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YqtKwASwg1n"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwDFKflJxCqp",
        "outputId": "8dd6a2f4-43f1-4d13-f44f-d45c59f46395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "url=tf.keras.utils.get_file('song_data.csv','https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "65290240/Unknown - 0s 0us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmvYV5LJ4AlQ"
      },
      "source": [
        "dataset=pd.read_csv(url)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_73vxnB94W4-"
      },
      "source": [
        "dataset=dataset[:1000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDtTc4CP4Yce"
      },
      "source": [
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus\n",
        "\n",
        "def tokenize_data(corpus,num_words=-1):\n",
        "  if num_words>-1:\n",
        "    tokenizer=Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEXLHBu-LKP"
      },
      "source": [
        "corpus=create_lyrics_corpus(dataset,'text')\n",
        "tokenizer=tokenize_data(corpus)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7_eX-ig-Lq4"
      },
      "source": [
        "total_words=len(tokenizer.word_index)+1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMxtwsV-xbM"
      },
      "source": [
        "token_list=tokenizer.texts_to_sequences(['look at her face its a wonderful face'])[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETENrXCJAVDY"
      },
      "source": [
        "sequences=[]\n",
        "for line in corpus:\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence=token_list[:i+1]\n",
        "    sequences.append(n_gram_sequence)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zro0yOTxAgG-"
      },
      "source": [
        "max_sequnce_length=max([len(seq) for seq in sequences ])\n",
        "padded_sequences=np.array(pad_sequences(sequences,maxlen=max_sequnce_length,padding='pre'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxNfciy8EcFN"
      },
      "source": [
        "input_sequences,labels=padded_sequences[:,:-1],padded_sequences[:,-1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpDiU7sHK8wP"
      },
      "source": [
        "one_hot_labels=tf.keras.utils.to_categorical(labels,num_classes=total_words)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLhVOiOMMC6E"
      },
      "source": [
        "model=tf.keras.models.Sequential([\n",
        "                            tf.keras.layers.Embedding(total_words,64,input_length=max_sequnce_length-1),\n",
        "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
        "                            tf.keras.layers.Dense(total_words,activation='softmax')\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLgmimebZVt"
      },
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goJsSYDkMQqP",
        "outputId": "0b2fd0cc-ee74-4ab8-9b0c-ba11a5f2dd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 19, 64)            598144    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 40)                13600     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9346)              383186    \n",
            "=================================================================\n",
            "Total params: 994,930\n",
            "Trainable params: 994,930\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FynfPm3jaRG_",
        "outputId": "a4b14821-7029-4ca0-a32f-e00753003fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(input_sequences,one_hot_labels,epochs=200,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 6.2535 - accuracy: 0.0573\n",
            "Epoch 2/200\n",
            "5760/5760 [==============================] - 75s 13ms/step - loss: 5.6391 - accuracy: 0.1049\n",
            "Epoch 3/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 5.3318 - accuracy: 0.1295\n",
            "Epoch 4/200\n",
            "5760/5760 [==============================] - 75s 13ms/step - loss: 5.1088 - accuracy: 0.1478\n",
            "Epoch 5/200\n",
            "5760/5760 [==============================] - 75s 13ms/step - loss: 4.9385 - accuracy: 0.1622\n",
            "Epoch 6/200\n",
            "5760/5760 [==============================] - 78s 14ms/step - loss: 4.8044 - accuracy: 0.1731\n",
            "Epoch 7/200\n",
            "5760/5760 [==============================] - 77s 13ms/step - loss: 4.6973 - accuracy: 0.1837\n",
            "Epoch 8/200\n",
            "5760/5760 [==============================] - 77s 13ms/step - loss: 4.6082 - accuracy: 0.1929\n",
            "Epoch 9/200\n",
            "5760/5760 [==============================] - 75s 13ms/step - loss: 4.5326 - accuracy: 0.2017\n",
            "Epoch 10/200\n",
            "5760/5760 [==============================] - 78s 14ms/step - loss: 4.4665 - accuracy: 0.2085\n",
            "Epoch 11/200\n",
            "5760/5760 [==============================] - 78s 13ms/step - loss: 4.4078 - accuracy: 0.2158\n",
            "Epoch 12/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 4.3566 - accuracy: 0.2212\n",
            "Epoch 13/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 4.3101 - accuracy: 0.2266\n",
            "Epoch 14/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 4.2678 - accuracy: 0.2321\n",
            "Epoch 15/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 4.2295 - accuracy: 0.2368\n",
            "Epoch 16/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 4.1943 - accuracy: 0.2411\n",
            "Epoch 17/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 4.1617 - accuracy: 0.2459\n",
            "Epoch 18/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 4.1259 - accuracy: 0.2503\n",
            "Epoch 19/200\n",
            "5760/5760 [==============================] - 72s 13ms/step - loss: 4.0912 - accuracy: 0.2547\n",
            "Epoch 20/200\n",
            "5760/5760 [==============================] - 72s 13ms/step - loss: 4.0577 - accuracy: 0.2586\n",
            "Epoch 21/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 4.0258 - accuracy: 0.2625\n",
            "Epoch 22/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.9943 - accuracy: 0.2675\n",
            "Epoch 23/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.9634 - accuracy: 0.2704\n",
            "Epoch 24/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.9335 - accuracy: 0.2745\n",
            "Epoch 25/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.9043 - accuracy: 0.2771\n",
            "Epoch 26/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.8758 - accuracy: 0.2809\n",
            "Epoch 27/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.8498 - accuracy: 0.2840\n",
            "Epoch 28/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.8240 - accuracy: 0.2867\n",
            "Epoch 29/200\n",
            "5760/5760 [==============================] - 72s 12ms/step - loss: 3.7997 - accuracy: 0.2900\n",
            "Epoch 30/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.7767 - accuracy: 0.2927\n",
            "Epoch 31/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.7559 - accuracy: 0.2953\n",
            "Epoch 32/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.7341 - accuracy: 0.2976\n",
            "Epoch 33/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.7140 - accuracy: 0.3001\n",
            "Epoch 34/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.6953 - accuracy: 0.3022\n",
            "Epoch 35/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.6792 - accuracy: 0.3046\n",
            "Epoch 36/200\n",
            "5760/5760 [==============================] - 73s 13ms/step - loss: 3.6616 - accuracy: 0.3066\n",
            "Epoch 37/200\n",
            "5760/5760 [==============================] - 74s 13ms/step - loss: 3.6433 - accuracy: 0.3093\n",
            "Epoch 38/200\n",
            "5760/5760 [==============================] - 70s 12ms/step - loss: 3.6304 - accuracy: 0.3108\n",
            "Epoch 39/200\n",
            "5760/5760 [==============================] - 80s 14ms/step - loss: 3.6143 - accuracy: 0.3135\n",
            "Epoch 40/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 3.6007 - accuracy: 0.3149\n",
            "Epoch 41/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 3.5854 - accuracy: 0.3168\n",
            "Epoch 42/200\n",
            "5760/5760 [==============================] - 76s 13ms/step - loss: 3.5718 - accuracy: 0.3191\n",
            "Epoch 43/200\n",
            "5760/5760 [==============================] - 77s 13ms/step - loss: 3.5603 - accuracy: 0.3207\n",
            "Epoch 44/200\n",
            "3761/5760 [==================>...........] - ETA: 26s - loss: 3.5270 - accuracy: 0.3246Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}